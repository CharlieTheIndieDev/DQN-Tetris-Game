AI INSTRUCTIONS

tensorboard --logdir=logs/ --host localhost --port 8088


What would happen if you penalize losses with the negative total reward of the whole game (if that is how it works)?

n Q-Learning, the best state is chosen based on the future rewards. So that's an interesting question, because the last state would always take away the entire reward. But since the discount value (i.e. how much the future reward is valued over the immediate one) is 0.95, there is some room to chose the best decision (although the results would probably not be as good, since it would make greedier decisions).

As for the -1 reward, it was just so it wouldn't be 0 (just to highlight the fact that it's a bad move, more from a logic standpoint instead of a learning one). Since the agent will always try to get the higher score, this means that even if the reward was positive, it would always try to go for a longer game.


DQN agent : 

# Deep Q Learning Agent + Maximin
#
# This version only provides only value per input,
# that indicates the score expected in that state.
# This is because the algorithm will try to find the
# best final state for the combinations of possible states,
# in contrast to the traditional way of finding the best
# action for a particular state.
# noinspection PyMethodMayBeStatic


"""Deep Q Learning Agent + Maximin

    Args:
        state_size (int): Size of the input domain
        mem_size (int): Size of the replay buffer
        discount (float): How important is the future rewards compared to the immediate ones [0,1]
        epsilon (float): Exploration (probability of random values given) value at the start
        epsilon_min (float): At what epsilon value the agent stops decrementing it
        epsilon_stop_episode (int): At what episode the agent stops decreasing the exploration variable
        n_neurons (list(int)): List with the number of neurons in each inner layer
        activations (list): List with the activations used in each inner layer, as well as the output
        loss (obj): Loss function
        optimizer (obj): Otimizer used
        replay_start_size: Minimum size needed to train
    """